---
title: "Group 10 Project 3"
date: "March 23, 2016"
author: "Weihan Li, Aoyuan Liao, Robert Minnich, Tara Shui, Yusen Wang"
output: html_document
---

## Cat and Dog Classification
For this project, our goal was to find an approach to classifying a collection of images featuring dogs and cats that was better than a currently existing baseline model. 

### Baseline Model
#### Linear SVM
<!--
Function used to find error rates:
```{r}
err.linear <- function(train.x, train.y, cost, K){
  ix <- sample(1 : length(train.y))
  begin <- 1 + seq(0, K - 1) * round(length(train.y) / K)
  end <- begin + round(length(train.y) / K) - 1
  len <- length(cost)
  err <- rep(0, len)
  for (i in 1 : len){
    for (k in 1 : K){
      test.ix <- ix[begin[k] : end[k]]
      train.ix <- ix[-(begin[k] : end[k])]
      m <- svm(train.x[train.ix, ], train.y[train.ix], type = 'C',
             kernel = 'linear', cost = cost[i],gamma = 0)
      y_hat <- predict(m, train.x[test.ix, ])
      err[i] <- err[i] + sum(y_hat != train.y[test.ix]) / length(train.y[test.ix])
    }
  } 
  err <- err / K
  return(err)
}
```
#### Error Rates
![image](./fig/Linear_SVM_Results.png)

This figure represents the resulting error rates after training and testing. We see that as the margin parameter increases, the error rate of classification increases as well, although not to a significant extent. Nonetheless, we could choose our optimal cost (lambda parameter) to be 25 if we were to continue using an SVM to classify these images.

-->
### Our Pipeline:
![image](./fig/Model_selection_plan.jpg)

#### Image processing
```{r}

```
<!--**Note: ** We used the full image to train and the full image to test throughout the project.-->

<div style = "text-align: center;">
  <span style="float:left;width: 40%;">
    <b>Full image</b>
  </span>
  <span style="float:left;width: 20%;">
    <b></b>
  </span>
  <span style="float:right;width: 40%;">
    <b>Boxed image</b>
  </span>
</div>

<div style = "width: 100%">
  <span style="float: left; width: 40%;">
    <IMG SRC="./fig/Abyssinian_102.jpg">
  </span>
  <span style="float: left; width: 20%;">
    <IMG SRC="./fig/arrow.png">
  </span>
  <span style="float: left; width: 40%;">
    <IMG SRC="./fig/Abyssinian_102f.jpg">
  </span>
  <br style="clear: left;" />
</div>

<div style = "text-align: center;">
  <span style="float:left;width: 40%;">
    <b>Full image</b>
  </span>
  <span style="float:left;width: 20%;">
    <b></b>
  </span>
  <span style="float:right;width: 40%;">
    <b>Boxed image</b>
  </span>
</div>

<div style = "width: 100%">
  <span style="float: left; width: 40%;">
    <IMG SRC="./fig/chihuahua_103.jpg">
  </span>
  <span style="float: left; width: 20%;">
    <IMG SRC="./fig/arrow.png">
  </span>
  <span style="float: left; width: 40%;">
    <IMG SRC="./fig/chihuahua_103f.jpg">
  </span>
  <br style="clear: left;" />
</div>

<div style = "text-align: center;">
  <span style="float:left;width: 40%;">
    <b>Full image</b>
  </span>
  <span style="float:left;width: 20%;">
    <b></b>
  </span>
  <span style="float:right;width: 40%;">
    <b>Boxed image</b>
  </span>
</div>

<div style = "width: 100%">
  <span style="float: left; width: 40%;">
    <IMG SRC="./fig/scottish_terrier_133.jpg">
  </span>
  <span style="float: left; width: 20%;">
    <IMG SRC="./fig/arrow.png">
  </span>
  <span style="float: left; width: 40%;">
    <IMG SRC="./fig/scottish_terrier_133f.jpg">
  </span>
  <br style="clear: left;" />
</div>
<br>&nbsp;<br> 

#### SIFT
We chose to use scale invariant feature transformation (SIFT) as the feature detector and descriptor for image analysis. 

The key challenge was to find a good balance between accuracy (# of keypoints/visual features) and time efficiency. The more keypoints we chose to keep, the higher the computational cost. But if we didn't keep enough keypoints, they wouldn't sufficiently represent the features we need to detect. For our base case, we decided on 25 keypoints per image.

<div style = "text-align: center;">
  <span style="float:left;width: 50%;">
  <IMG SRC="./fig/No_Edge_Detector.png" float = "right" ALT="image">
  </span>
  <span style="float:right;width: 50%;">
  <IMG SRC="./fig/Edge_Detector.png" float = "left" ALT="image">
  </span>
</div>
<div style = "text-align: center;">
  <span style="float:left;width: 50%;">
    <b>Sift with no edge detection</b>
  </span>
  <span style="float:right;width: 50%;">
    <b>Sift with edge detection</b>
  </span>
</div>
<br>&nbsp;<br> 

To find the most optimal number of keypoints to work with, we arbitrarily chose the cluster size to be 25 and made a straightforward comparison of various machine learning algorithms and their performance levels between 0 and 400 keypoints. We found that a sufficient number of algorithms hit high points of accuracy at keypoints = 300, and thus chose to use 300 moving forward.

<div style = "text-align: center;">
  <span style="float:left;width: 100%;">
  <IMG SRC="./fig/Keypoints_Acc_300.png" float = "right" ALT="image">
  </span>
</div>
<br>&nbsp;<br> 

#### K-Means Clustering + Binning
We chose to use k-means clustering to produce the visual vocabulary (codebook) because other better-performing clustering algorithms are often too slow and too memory-intensive. K-means sufficiently reduces the space of the data in a relatively fast and robust way.

<!--**Number of clusters = 150**-->

Furthermore we used a variation of k-means in which we took mini-batches, i.e. MiniBatchKMeans, to reduce computation time but largely maintain similar results. The figure below shows an example of how the algorithm compares to using k-means.

<div style = "text-align: center;">
  <span style="float:left;width: 100%;">
  <IMG SRC="./fig/Mini_Batch_Kmeans.png" float = "right" ALT="image">
  </span>
</div>
<br>&nbsp;<br> 

Setting keypoints = 300, we again looked at the optimal number of clusters to use and found that many of the machine learning algorithms reached a performance peak at # of clusters = 150. Note that at # of clusters = 300, the performance of each algorithm generally stabilizes. This is intuitive since each cluster would only have one keypoint and so any number of clusters above 300 is trivial. (Also note that we are not sure why SVM in this situation behaved the way it did.)

<div style = "text-align: center;">
  <span style="float:left;width: 100%;">
  <IMG SRC="./fig/Cluster_Acc_25.png" float = "right" ALT="image">
  </span>
</div>
<br>&nbsp;<br>


#### Model selection + Testing

#### Final Model

#### Performance improvement
Average Accuracy for SVM: 0.68024585130518866

#### Running cost tradeoff
Average Train, Test time for SVM: 0.44722166061401369

Average time for SIFT : 254.54186606407166

Average time for Kmeans: 2.0315157890319826

### Conclusion

